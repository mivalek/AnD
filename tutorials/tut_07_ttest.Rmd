---
title: "Tutorial 6: Comparing Two Means"
author: "Analysing Data"
---

```{r setup, include=FALSE}
library(tidyverse)
library(here)
library(kableExtra)
library(cowplot)
library(weights)
library(Hmisc)
knitr::opts_chunk$set(echo = T, fig.height=4, fig.width=5, fig.align = "center", message = F, warning = F, toggle = F)
```

```{r, include = F}
report.p <- function(x){
  ifelse(x >= .001, paste0("= ", rd(x,3)), "< .001")
}
```

This tutorial covers how to prepare for, complete, and report *t*-tests in R. Before you begin this tutorial, make sure you review the [Week 6 lecture](https://canvas.sussex.ac.uk/courses/9242/pages/week-6?module_item_id=612284) covering comparing two means. This tutorial will do this two ways: first as a *t*-test, which is what you should do for your lab report; and second as a linear model. This will help demonstrate how these two analyses are actually capturing the same relationship. 

## Which Analysis?

If you are planning to write about [the red study (Elliot et al., 2010)](https://canvas.sussex.ac.uk/courses/9242/pages/Lab%20Report%20Information%20and%20Resources#red) for your lab report, this is the tutorial that will teach you the analysis to use. If you are planning to write about [the green study (Griskevicius et al, 2010)](https://canvas.sussex.ac.uk/courses/9242/pages/Lab%20Report%20Information%20and%20Resources#green), then you should review [last week's tutorial and practical on chi-squared](https://canvas.sussex.ac.uk/courses/9242/pages/week-6) instead.

## Setting Up

Just like last time, all you need for this tutorial is this file and RStudio. Remember that you can easily switch between windows with the <kbd>Alt</kbd> + <kbd>&#8633; Tab</kbd> (Windows) and <kbd>&#8984;\ Command</kbd> + <kbd>&rarrb; Tab</kbd> (Mac OS) shortcuts.

`r task()`Open your analysing_data `R` project in RStudio and open a new Markdown file. Since we will be practicing reporting using inline code, we will need Markdown later on. For the tasks, get into the habit of creating new code chunks as you go.

\ 

`r task()`Aside from loading `tidyverse`, we will also be using `Hmisc`, `here`, `cowplot`, and `kableExtra`. If you don't have any of these packages installed, do that now. **Remember that installing packages is a one-off thing** so don't put the command that does it in your Markdown. Simply type it into the console and press <kbd>&crarr;\ Enter</kbd>. The `library()` commands should always go in a separate code chunk at the beginning of your Markdown document, so that your code will run correctly.

```{r, eval = F, toggle = T}
install.packages("Hmisc")
library(tidyverse)
library(Hmisc)
library(here)
library(kableExtra)
library(cowplot)
```

\ 

As usual, we will need some data to work with. Let's have another look at last week's practical data, `sync`.

`r task()`Read in the `sync` data ([which you can use this link to download](http://users.sussex.ac.uk/~jm636/synchrony_short.csv)) and copy over the code you wrote for [last week's practical](http://users.sussex.ac.uk/~jm636/prac_05_wkst_sol.html) to clean it. If you haven't finished this yet, you may want to do this first, or work through the posted solutions, before you continue with this tutorial.

```{r, toggle = T}
sync <- read_csv("http://users.sussex.ac.uk/~jm636/synchrony_short.csv")

# Data checking
## Remove problematic value and recode age
sync <- sync %>% 
  mutate(Age = recode(Age, "3p" = "300"),
         Age = as.numeric(Age))

## Recode factor variables as factors with labels
sync <- sync %>% 
  mutate(Px = factor(Px),
         Gender = factor(Gender, labels = c("Male", "Female")),
         condition = factor(condition, labels = c("Synchrony", "Asynchrony")))

# Data cleaning
## Remove improbable values of age
## Don't forget to save the number you will remove before you do it!

age_removed <- sync %>% filter(Age > 99) %>% nrow()

sync <- sync %>% 
  filter(Age < 100)

# Replace impossible score with NA
# Using na_if()
sync <- sync %>% 
  mutate(nars3 = na_if(nars3, 25))

## Remove participants with all NAs
# and save how many we'll remove before we do so.
na_removed <- sync %>% filter(rowSums(is.na(sync)) > 5) %>% nrow()

sync <- sync %>% 
  filter(rowSums(is.na(sync)) < 6)

```

\ 

## Data Preparation

As usual, before beginning any analysis, we will want to take a look at our data to make sure it's clean and correct before we proceed. Since we're using our familiar `sync` dataset that we've cleaned already, we should be pretty happy with this, but let's have a look again at these variables. If you don't remember what your variables measure, then the numbers you get from your analysis won't mean anything!

`r task()`Review the codebook below and compare to your data using any method you like. If there are any differences or problems, fix them before you proceed.

### Codebook

```{r, echo = F}
read_csv("../data/synchrony_codebook.csv") %>% 
  kable() %>% 
  kable_styling()
```

```{r, eval = F, toggle = T}
# Use one or (better!) all of the following
# to have a look at your data 
# Run just one at a time by putting your cursor 
# anywhere inside the command and pressing ctrl + Enter
# or cmd + Enter on Macs

# Open the dataset in the Viewer, or just click on the object in Environment
View(sync) 

# Create a summary of each variable
sync %>% summary()

# Look at the structure of the dataset
sync %>% str() 

# Print out a preview of the tibble in the console
sync 
```

Thus far, everything looks good. We can see from the codebook that we have three ratings of people's opinions of Pepper (`liking`, `anth`, and `perc.int`) and three measures on the NARS (`nars1`, `nars2`, `nars3`). We can take a look at these measures individually, but it would also be useful to create a **composite score** out of these individual ratings.

## Creating a Composite Score

Composite scores can be created in different ways, but we're just going to use perhaps the easiest: the mean. What we would like to do is calculate a new variable that is the mean of each person's scores on the three opinion scales (i.e. `liking`, `anth`, and `perc.int`). These three questions are all different aspects of people's opinions about Pepper and they're all on the same scale (i.e. 0 - 5), so we can take the mean of the three ratings that each person gave and create a new variable, `opinion_comp`, on the same scale that captures all three ratings together.

### Attempt 1: `c()`

To do this, we might first try the following, using the following familiar code. Here, we put `liking`, `anth`, and `perc.int` into `c()` because we want all three of them to go in the first argument of `mean()` together. See the box further down about using `c()` if you're not sure how this works.

```{r comp_wrong}
sync %>% 
  mutate(opinion_comp = mean(c(liking, anth, perc.int))) %>% 
  # You don't need the line below,
  # it's only to make it easier to quickly find the new variable
  select(Px, liking, anth, perc.int, opinion_comp)
```

This works...or at least, it runs (not the same thing!). If we look at the new variable at the end of our `sync` tibble, we can see that `opinion_comp` has been created, but there's nothing in it but NAs! What happened?

Thinking back to our summary, we might remember that there were some NAs in our data. As long as there is at least one NA in at least one of the variables, `mean()` will return NA. However, we might not have known this unless we checked! R didn't give us an error because as far as R was concerned, it did what we asked it to do without any problems.

### Attempt 2: `na.rm`

Let's try again, this time telling R to ignore any NAs using `na.rm = T`:

```{r comp_wrong2}
sync %>% 
  mutate(opinion_comp = mean(c(liking, anth, perc.int), na.rm = T)) %>% 
  select(Px, liking, anth, perc.int, opinion_comp)
```

At first glance, this seems fine - we have numbers in `opinion_comp` now. But on closer look, we can see that they're all the same number, even though we can see from the tibble printout that each row has different values in it. What happened?

This time, R has again quietly done what we've asked. We asked it to calculate the mean of our three variables, and store that value in `opinion_comp`. What R didn't do (because we didn't tell it to!) is calculate a new mean **for each case**. Instead, R calculated the **overall mean** of all of the numbers in all three variables and stored that one value for every case. This is not what we wanted, but again, R doesn't know that; it's just done what we asked it to do.

### Attempt 3: `group_by()`

Right, what we really want is that `opinion_comp` will contain the mean of the three ratings calculated for each person individually. We've solved a similar problem before using `group_by()`. Let's try grouping by `Px`, our participant code, which we've used previously in descriptive tables for a similar purpose.

```{r comp3}
sync %>% 
  group_by(Px) %>% 
  mutate(opinion_comp = mean(c(liking, anth, perc.int), na.rm = T)) %>% 
  select(Px, liking, anth, perc.int, opinion_comp)
```

Aha, third time lucky! Now we can see that there are different numbers for each person in `opinion_comp`. If we're feeling a bit paranoid now that R's done something we weren't expecting a few times, we can check that this calculation is what we think it should be by choosing a random person and calculating the mean for ourselves, using R as a calculator. Looking at our tibble above, we can see our participant with ID code 7 (i.e., `Px` = 7) gave `liking`, `anth`, and `perc.int` ratings of 5, 4.4, and 4.6 respectively. Let's do the mean calculation ourselves:

```{r check_mean}
(5 + 4.4 + 4.6) / 3
```

The result is the same number that is stored for participant 7 in `opinion_comp`. 

### Success!

Now that we have code that works, we can actually create this variable in our dataset by using `<-` to assign it back into the same object. This will overwrite our original `sync` dataset with our new one, which is identical except for the new variable `opinion_comp`.

`r task()`Make the change to your dataset `sync`. When you do, you  must include one more important element. After we create `opinion_comp`, we must remember to **ungroup** our dataset again. If we don't, we may get weird results down the line, because R will try to make plots or do analyses for every participant individually! We can do this easily with `ungroup()`.

```{r, toggle = T}
sync <- sync %>% 
  group_by(Px) %>% 
  mutate(opinion_comp = mean(c(liking, anth, perc.int), na.rm = T)) %>% 
  ungroup()
```

`r task()`Create two new variables in the `sync` data:

* `nars_total`, which is the sum of the scores on the three NARS scales
  + Hint: try `?sum`
* `nars_comp`, which is the mean rating across all three NARS scales
  + Refer to the codebook to figure out how many questions you need to divide by!  

```{r, toggle = T}
sync <- sync %>% 
  group_by(Px) %>% 
  mutate(nars_total = sum(nars1, nars2, nars3),
         nars_comp = nars_total/14) %>% 
  ungroup()

# Call the dataset to see the new variables
sync

```

\ 

<details>
<summary>Further Explanation</summary>
Each argument within `mutate` creates the two new variables for this task. The first uses `sum()` to add together the scores on the three NARS scales. We can see by asking for a summary that `nars_total` now contains scores ranging from `r sync %>% pull(nars_total) %>% min(na.rm = T)` to `r sync %>% pull(nars_total) %>% max(na.rm = T)`:

```{r}
sync %>% 
  pull(nars_total) %>% 
  summary()
```

Referring again to the codebook, we can see that the three NARS scales were measured on a scale of 1 - 5, and each contain different numbers of questions: 6, 5, and 3 respectively. So, to find the mean rating on these three scales, we first need to add 6, 5, and 3 to find the total number of questions on the NARS. You can do this in your head or use R:

```{r}
6 + 5 + 3
```

This means that to find the mean NARS score, we have to divide by 14, because there were 14 questions total and we want the mean score per question. So, we can't use `mean()`, because that will divide by the number of variables we put into it (i.e. three: `nars1`, `nars2`, and `nars3`). Instead we can use the total score across the three scales we calculated in the first step, `nars_total`, and divide by the number of questions on the NARS altogether, which is 14, to get the mean per question.

**Note:** The creator of the NARS scales do not recommend averaging across the scales like this! So we'll do it for practice, but you should always research your scales carefully before you start transforming them like this.
</details>

\ 

<div class = "why">
#### Using `c()`

We have come across the humble `c()` many times already, both this semester and last semester. `c()` stands for "concatenate", or "collect", or "combine", or something to that effect. The idea is that you can put any number of things into `c()`, and they will be combined into a single vector (basically a series).

This is a useful thing to be able to do because of the way arguments work in R. Let's have a look at the task we just did using `mean()`. Before we start, type `?mean` or `help(mean)` into your console and press Enter to see the documentation for this function.

From the documentation we can see that `mean()`'s first argument, `x`, has to contain everything you want to take the mean of. It also has more arguments, `trim` and `na.rm` and so on, some of which we have used before. In the composite score example above, though, we had three variables that we wanted to take the mean of. Let's have a look at what would happen without `c()` by running the following code:

```{r, eval = F}
sync %>% 
  group_by(Px) %>% 
  mutate(opinion_comp = mean(liking, anth, perc.int))
```
The error that you'll get doesn't seem relevent: what is `if (trim...)`, and why TRUE/FALSE? This may look at first glance like complete nonsense, but it is actually R telling you what's gone wrong. Because we've been getting the hang of using these functions, we haven't been writing out the argument names if we don't need to. R, very sensibly, will take each object that you put into a function and try to use each one in the same order as you put it in. So when we wrote this:

```{r, eval = F}
mean(liking, anth, perc.int)
```

what we actually asked R to do was this (as we can see from the help documentation):

```{r, eval = F}
mean(x = liking, trim = anth, na.rm = perc.int)
```

Understandably, R has complained about this; `trim = anth` is nonsense and not at all what we wanted, much less `na.rm = perc.int`! Instead, we need to tell R that we want all three variables, together, to go in the `x` argument. This is where `c()` comes in. It allows us to put multiple things into one argument together, so instead we can write:

```{r, eval = F}
mean(c(liking, anth, perc.int))
```

and R will understand that we want the mean of all three of those variables, because they are all together in the `x` argument.

Overall `c()` is terribly useful and pops up all over the place. You should use it whenever you need to give R more than one thing at a time, which is often the case. Now when you see it in our examples, try to work out what it's doing and why - and remember to use it yourself!
</div>

## Graphing

Before we jump into calculating numbers, we should get an idea of what our data look like. This will help us better understand the numbers we get from our analysis.

### Histograms

The first thing we should do is have a look at some histograms to get a sense of how people responded to these questions. Let's start by looking at histograms of all of our rating variables. If you don't remember `pivot_longer`, have a look at last week's tutorial for a reminder.

`r task()`Copy the code below and replace the ellipses `...` with the correct elements. For `select()`, only keep the three opinion ratings and their composite mean, and the composite mean of the NARS scales.

```{r, eval = F}
 sync %>% 
  select(...)
  pivot_longer(everything(), names_to = "variable", values_to = "value") %>% 
  ggplot(...(value)) +
  ...() +
  ...(~variable) +
  theme_...()
```

```{r, fig.width = 10, toggle = T}
sync %>% 
  select(liking, anth, perc.int, opinion_comp, nars_comp) %>% 
  pivot_longer(everything(), names_to = "variable", values_to = "value") %>% 
  ggplot(aes(value)) +
  geom_histogram() +
  facet_wrap(~variable) +
  # You don't have to use cowplot - try different themes if you like!
  theme_cowplot()
```

Here we can get a sense of how our data is distributed in each variable. It looks like people tended to like Pepper the robot a lot, since the scores tend to be piled up on the high end of the scale in `liking`; but they didn't think she was very human-like, which gives us the reverse pattern in `anth`. They did think she was reasonably intelligent (`perc.int`), but the scores here are a bit more symmetrical than in `liking`. The distribution of our composite scores, `opinion_comp` and `nars_comp`, are a bit more centred in the scale and symmetrical.

`r task()`**Challenge**: Based on the definitions (from the codebook) and distributions of the `liking`, `anth`, and `perc.int` variables, do you think it's actually a good idea to calculate and use the `opinion_comp` variable? How could you investigate this using the data and techniques we've covered so far? Have a go at quantifying and visualising the relationships between these variables, and write down your opinion.

<details>
<summary>Use the composite score or not?</summary>

Composite scores are often used when you have several different values or ratings that all (supposedly, at least) measure the same thing, or different aspects of the same thing. It only makes sense to calculate and use a composite score if you think this is the case. The question we are really asking here is: are ratings of liking, anthropomorphism, and percieved intellingence really all different aspects of the same thing? Does averaging them together create a meaningful single measure of opinion, where a low number means a more negative and a higher number a more positive opinion?

The distribution of the variables suggest that that might not be the case. Liking, for instance, maps onto this idea of general opinion fairly well. But is it the case that if you think something is more human-like (i.e. anthropomorphic), you also have a higher opinion of it? I personally have a very high opinion of dogs, but I don't think they're very human-like. (It is, in fact, one of the many things I like about them!) To investigate this, we could ask for a correlation of `liking` and `anth` to see if they are in fact related to each other systematically:

```{r cor}
cor.like.anth <- cor.test(sync$liking, sync$anth)
cor.like.anth
```

As we learned last week, this correlation is significant. Is that the end of it? Not necessarily - this just tells us that people who liked Pepper more also tended, on the whole, to rate her as more human-like. A correlation of `r cor.like.anth$estimate %>% round(2)` isn't particularly strong, though. Let's visualise this with a scatterplot; the code should all be familiar to you.

```{r}
sync %>% 
  ggplot(aes(liking, perc.int)) + 
  geom_point(position = "jitter") +
  labs(x = "Rating of Liking", y = "Rating of Anthropomorphism") +
  theme_cowplot()
```

Although a significant correlation seems impressive, we can see from the plot that the dots are scattered quite a bit, and don't show a very obvious pattern. This doesn't give me much confidence that combining `liking` and `anth` into a single score is a good idea. What about the other correlations within the `opinion_comp` variable? Let's make things a bit easier and get a correlation matrix, as we learned last week:

```{r}
sync %>% 
  select(liking, anth, perc.int) %>% 
  as.matrix() %>% 
  rcorr()
```

As we can see in the correlation matrix in the top table, it looks like `anth` has a medium-strength correlation with `perc.int`, and `perc.int` has a similarly medium-strength correlation to `liking`. All of these correlations are significant, but again, that's not the end of it. Typically, variables that are "measuring the same thing" are expected to correlate around .8 and above, and none of these are at all close.

What we are really discussing is *operationalisation* - how do we measure something, and how can we be sure that the numbers we use actually correspond to the constructs we're interested in? This isn't an easy question, but it's an important one! Overall my personal opinion is that in this particular case, these three variables are not really measuring the same thing, and it's better to analyse them separately rather than as a composite. You're welcome to have a different opinion, of course - these are exactly the kinds of questions you will need to deal with in, for instance, your third year project. If you do have a different opinion, though, you must be able to back it up with evidence!
</details>

### Means Plots

When you are analysing and presenting data comparing two means, it is typical (and sensible!) to visualise this relationship by presenting plots of the means. We'll go through the steps of creating a means plot with error bars below.

`r task()`We'll start by looking at one of the main hypotheses of the original study: namely, that people in different conditions would have different mean ratings of how much they like Pepper. 

`r subtask()`Use `ggplot()` to map the variables that we will use, add nice labels, and choose a theme. Save this as `like.plot` and call it to have a look.

```{r, toggle = T}
like.plot <- sync %>% 
  ggplot(aes(x = condition, y = liking)) +
  labs(x = "Synchrony Condition", y = "Mean Rating of Liking") +
  theme_cowplot()

like.plot
```

So, now we have a nice blank canvas on which we can draw our plot. One of the ways to make the plot-building process easier is to set up and save the elements of the plot you are sure of, as we've just done. Then, we can just add more layers to the plot using `+`.

`r subtask()`Add a geom to your plot to draw points on it, jittered a bit.

```{r, toggle = T}
like.plot +
  geom_point(position = "jitter")
```

We now have a scatterplot that gives us an idea of how our data are distributed in each condition. This isn't quite what we want, though - we don't want to plot the data itself, but rather the mean in each condition. To do this, we need to tell R that we want a summary of the data, rather than the data itself; and that the summary we want is the mean.

`r subtask()`Add the following code to your plot to create means in each condition.

```{r, eval = F}
geom_point(stat = "summary",
           fun.y = "mean")
```

```{r, toggle = T}
like.plot + 
  geom_point(stat = "summary",
           fun.y = "mean")
```

Huh. That's what we wanted, but it looks like the difference in means is enormous. Based on the scatterplot we saw above, it didn't look like there would be a big difference. If we take a look at the *y*-axis, we can see why: R has changed the scale to make the difference between the points look huge. R is just trying to be helpful, but this misrepresents the actual size of the difference and makes the plot very difficult to interpret. Let's override that automatic adjustment and choose our own scale. Since the ratings of `liking` ranged from 0 to 5, let's set our *y*-axis to the same values.

`r subtask()`Add the following code to change the limits on the *y*-axis by replacing `...` with numbers.
```{r, eval = F}
ylim(...)
```

```{r, toggle = T}
like.plot + 
  geom_point(stat = "summary",
           fun.y = "mean") +
  ylim(0, 5)
```

That's more like it. Those single black dots are pretty boring, though - let's pick something more interesting.

`r subtask()`Add the following arguments to `geom_point()` to change the size and shape of the points. Each argument should be completed with a number.

**Hint:** if you choose a number for `shape =` between 21 and 25, you can also add an extra argument, `fill = `, which you can complete with a colour. Some helpful websites:

* [Shape options](http://sape.inf.usi.ch/quick-reference/ggplot2/shape)
* [Shapes and colours](http://www.sthda.com/english/wiki/ggplot2-point-shapes)

```{r, eval = F}
size = ...
shape = ...
```

```{r, toggle = T}
like.plot + 
  geom_point(stat = "summary",
           fun.y = "mean",
           size = 5, # chosen at random - larger numbers make bigger points
           shape = 23,
           fill = "red") +
  ylim(0, 5)
```

Now that's looking a bit better. The last thing we want here is some measure of the variance around the mean. Since we've talked about them a lot, and we'll see them in our output later, let's put some error bars around our means that represent 95% confidence intervals. 

`r subtask()`Add the following code to your plot to create error bars.

```{r, eval = F}
stat_summary(fun.data="mean_cl_boot",geom="errorbar")
```

```{r, toggle = T}
like.plot +
  geom_point(stat = "summary",
           fun.y = "mean",
           size = 5, # chosen at random - larger numbers make bigger points
           shape = 23,
           fill = "red") +
  ylim(0, 5) + 
  stat_summary(fun.data="mean_cl_boot",geom="errorbar") 
```

We're almost there! One, I think those error bars are a little Extra (tm), so let's make them a little less [Lady Gaga](https://www.google.com/search?q=lady+gaga+shoulder+pads) using `width = ` in the `stat_summary()` function. We can also get rearrange the order of the layers so the points are drawn on top of (i.e. after) the error bars, instead of underneath.

`r subtask()`Create the finished plot and save it as `like.plot.final`. Then, call it to have a look.

```{r, toggle = T}
like.plot.final <- like.plot + 
  stat_summary(fun.data="mean_cl_boot",geom="errorbar", width = .25) +
  geom_point(stat = "summary",
           fun.y = "mean",
           size = 5, # chosen at random - larger numbers make bigger points
           shape = 23,
           fill = "red") +
  ylim(0, 5)
like.plot.final
```

Finally, we can interpret this plot. We can see that the means of liking in the two synchrony conditions are almost identical, although the confidence intervals are slightly different. This tells us that synchrony condition seemed to make little difference to how much participants liked Pepper, although we'll need to investigate this with a test to be sure.

`r task()`Make a similar plot with the following specifications:

* `Gender` on the *x*-axis and `nars1` on the *y*-axis
* Correct labels and scales on both axes
* Plot means as teal circles
* Save as `nars.plot.final`

```{r, toggle = T}
nars.plot.final <- sync %>% 
  ggplot(aes(x = Gender, y = nars1)) +
  labs(x = "Participant Gender", y = "Mean NARS Subscale 1 Score") +
  stat_summary(fun.data="mean_cl_boot",geom="errorbar",width = .25) +
  geom_point(stat = "summary",
           fun.y = "mean",
           size = 5, # chosen at random - larger numbers make bigger points
           shape = 21,
           fill = "#008080") + # or whatever colour you chose!
  ylim(0, 30) + # Don't forget to change this!
  theme_cowplot()

nars.plot.final
```

## Comparing Means with `t.test()`

Now that we have some idea of what's happening in our data, let's quantify these relationships with an analysis!

Just like we saw last week with `cor.test()`, `t.test()` produces (relatively) clear output and also lets us report our results easily using subsetting, since it produces an `htest` object just like the ones we used for reporting before. So, we can use all the same ideas to run and report our *t*-test.

`r task()`Open the help documentation for `t.test` and have a look under "Usage" and "Arguments" at how to use it.

We can use the following arguments for `t.test`:

* `x` and (optionally) `y` are vectors of numeric data - i.e., the two variables we want to compare the means of. However, instead we'll be using...
* `formula` is a (more) mathematical representation of the test you want to run - see below for how to use it!
* `alternative` must be set to one of the three options `"two.sided"`, `"less"`, or  `"greater"`. This determines the type of alternative hypothesis. Because we are going to be using two-tailed tests, we will stick with `"two-sided"`.
* `paired` is set to `FALSE` by default. This is fine for the design we have, but if we had repeated measures, we would have to change this to `TRUE.`
* `var.equal` is set to `FALSE` by default. To get our "standard" *t*-test, we have to change this to `TRUE`. See the optional aside, after the analysis tasks, about what this argument means.
* `...` are other options you can change; we'll stick with the defaults for now.

### Using `formula`

The `formula` argument for `t.test()` is a new one, but in this case, it's very simple. We're using `formula` instead of `x` and `y` as before because it's easier, and it will help us with `lm()` below and for the next few weeks of this module (and next year!).

For this *t*-test, we would like to test whether people in different conditions rated Pepper differently. (You may recall from last week that the original study hypothesized that people in the Synchrony condition would like Pepper more than people in the Asynchrony condition.) In other words, we want to know whether `liking` changes depending on `condition`. We can translate this into a formula for R by simply inserting the operator `~`, which you can read as "changes depending on." So, we can write our formula in this case as `liking ~ condition`.

`r task()`Using what you've learned so far, try running a *t*-test comparing the means of `liking` in the two `condition` groups and save the output as `like.t`.

```{r like_t, toggle = T}
like.t <- t.test(liking ~ condition, data = sync, alternative = "two.sided", var.equal = T)

# We can also use a pipe to do this - note the . to tell R where to pipe the dataset
sync %>% 
  t.test(liking ~ condition, ., alternative = "two.sided", var.equal = T)
```

When we run this function, the output is some reasonably easy to read information about the results of our test. We can see here that we have:

* The type of analysis we asked for (Two Sample *t*-test) and the variables we used
* The values of *t*, degrees of freedom, and *p*
* The 95% confidence interval around the difference in means
* The means of each group

`r subtask()`Stop and look at the values that the output has given you. Based on what you have learned from the lecture, write down what you think this result means, and how you could interpret it.

<details>
<summary>Interpretation</summary>
First, we can see that there is extremely little difference between the means - they were almost exactly the same in the two conditions. We also saw this when we made our means plots earlier, so this shouldn't be a surprise. Accordingly, the value of *t* is very small; this means that the difference in the means is tiny compared to how much we might expect the differences between means to vary. In other words, there isn't much signal compared to the amount of noise!

As we saw in the lecture, to quantify this, we find the probability of obtaining this value for *t* if the true difference between the means is 0 - that is, if `condition` has no effect on `liking`, and these means of `liking` actually come from the same population. We do this using a *t*-distribution with the degrees of freedom given in the output - namely, `r like.t$parameter`. As we can see, it is extremely likely - almost certain, in fact! - that we would find a value of *t* this big (or bigger) if there was no real difference in the means, because *p* `r like.t$p.value %>% report.p()`. We can conclude from this that we should retain the null hypothesis, that synchrony condition makes no difference to how much people liked Pepper.

Finally, we can also have a look at the confidence intervals. These are confidence intervals around the difference in means, which is not explicitly stated in the output. However, we can calculate it by simply finding the difference between the two means, which is `r like.t$estimate[1] %>% round(2)` - `r like.t$estimate[2] %>% round(2)` = `r (like.t$estimate[1] - like.t$estimate[2]) %>% round(2)`. That's pretty tiny on our rating scale of 0 - 5! The confidence intervales give us a range of values that, for 95% of intervals, will contain the true population value. Because we are looking at the **difference** in means, we should check whether these confidence intervals cross or include 0. This is because a difference of 0 means that the means are exactly the same (which you might recognise as our null hypothesis). We can already see that the difference is means that we just calculated is barely different from 0 anyway; but we can also see that the lower CI is negative, and the higher CI is positive. This means that 0 lies within the confidence interval, so the true difference in means in the population could in fact be 0.
</details>

`r subtask()`Using what you learned about subsetting `htest` objects last week, copy the following paragraph into your Markdown document and report the results of this analysis as demonstrated in the lecture by filling it in with inline code. If you prefer, you are welcome to write the results in your own words instead.

**Hint**: if you can't remember what the elements are called to use in your reporting, try using `str()`, or look under "Values" in the help documentation.

> We investigated whether, as predicted, people would rate their liking of Pepper differently depending on synchrony condition using an independent-measures *t*-test. This test showed that the means in the two conditions were almost identical (*M*~diff~ = [code], 95% CI [ [code], [code]]) and there was no significant difference between the two groups on ratings of liking (*t* ([code]) = [code], *p* = [code]).

<details>
<summary>Solution</summary>

<pre><code>
We investigated whether, as predicted, people would rate their liking of Pepper differently depending on synchrony condition using an independent-measures *t*-test. This test showed that the means in the two conditions were almost identical (*M*~diff~ = &#96;r (like.t$estimate[1] - like.t$estimate[2]) %>% round(2)&#96;, 95% CI [&#96;r like.t$conf.int[1] %>% round(2)&#96;, &#96;r like.t$conf.int[2] %>% round(2)&#96;]) and there was no significant difference between the two groups on ratings of liking (*t* (&#96;r like.t$parameter&#96;) = &#96;r like.t$statistic %>% round(2)&#96;, *p* = &#96;r like.t$p.value %>% rd(.,3)&#96;).
</code></pre>

This should appear in Markdown as:

We investigated whether, as predicted, people would rate their liking of Pepper differently depending on synchrony condition using an independent-measures *t*-test. This test showed that the means in the two conditions were almost identical (*M*~diff~ = `r (like.t$estimate[1] - like.t$estimate[2]) %>% round(2)`, 95% CI [`r like.t$conf.int[1] %>% round(2)`, `r like.t$conf.int[2] %>% round(2)`]) and there was no significant difference between the two groups on ratings of liking (*t* (`r like.t$parameter`) = `r like.t$statistic %>% round(2)`, *p* = `r like.t$p.value %>% rd(.,3)`).

**Note:** I'm using `rd()` from the `weights` package instead of `round()` for the *p*-value because it drops the leading 0, which is what I want; `round()` does not do this. For more about reporting *p*-values using inline code, see the [Creating Functions section in last week's tutorial](http://users.sussex.ac.uk/~jm636/tut_05_cor_chisq.html#bonus:_creating_functions)
</details>

<div class = "why">
### Using `var.equal`

You may have noticed that the only argument in `t.test()` that we have to change from the default is `var.equal`, which we are setting to `TRUE`. This argument specifies whether the variances of the two samples from which we are getting our means are equal. This is an important assumption of the standard *t*-test, but we haven't talked much about assumptions yet this year. If you're interested in a brief preview of some of next year's topics, read on.

Essentially, the *t*-test - or, indeed, any statistical test - only works the way we want it to if its assumptions are met. We ran into the idea of assumptions before with chi-squared, where the expected frequencies had to be bigger than 5 in each cell. You can think of assumptions like the prerequisites on a module. If you don't meet the prerequisites, the module will keep going without you, but you might not get the marks you want! In the same way, statistical tests have assumptions, and they only work properly - by which I mean, give you a reliable answer - when those assumptions have been met.

At the moment, we're not going to worry too much about assumptions, since it's more important for you to understand the idea of the test itself and how to use it. If you're interested, though, you can set `var.equal` to `FALSE`, or delete the argument from your command (which will default to `FALSE` anyway). If you do this, you'll get a slightly different output:

```{r like_var}
sync %>% 
  t.test(liking ~ condition, ., alternative = "two.sided", var.equal = F)
```

You might notice a few differences. First, the title: we now have a "Welch" *t*-test. You might also notice that the numbers have changed slightly, even though the overall result is the same. What we have done is tell R that we can't assume equal variances in our two samples. So, R has run a slightly different test that makes a mathematical adjustment for any differences between the two groups. Of course, if we just always used this adjusted test, we would never need to worry about equal variances at all...but that's a topic for next year!
</div>

`r task()`Now that you've had some step-by-step practice, try doing the entire analysis yourself. Find out whether scores on the first of the NARS scales were different between male and female participants. When you do this, you should:

* Run the analysis using `t.test()`
* Look through the results to understand what they mean
* Write up and report the results in your own words using inline code

<details>
<summary>Running and Interpreting the Analysis</summary>

To begin, you have to write out your formula. Here, our predictor is `Gender`, and our outcome is `nars1`. Although it seems a little backwards, we must write this as *outcome ~ predictor(s)*, which here would be `nars1 ~ Gender`. Everything else is the same.

```{r, toggle = T}
# Make sure you save the result in a new object
# You can name the new object anything you like,
# but it's a good idea to call it something informative (not "result")
nars.t <- sync %>% 
  t.test(nars1 ~ Gender, ., alternative = "two.sided", var.equal = T)
nars.t 
```

Here we can see that male participants had a lower total score on the first NARS scale than female participants. We can see that this difference in means was just over two times bigger than the standard error of the differences in means (using our value of *t*). For `r nars.t$parameter` degrees of freedom, a difference in means this big or bigger is fairly unlikely to happen if the true difference in means is in fact 0. Since *p* `r nars.t$p.value %>% report.p()`, which is less than .05, we conclude that we should reject the null and accept the alternative - namely, that male and female participants differed on the first NARS scale. The confidence intervals give us more evidence of this, because while the upper bound is quite close to 0, they are both below 0. That tells us that if this interval is one of the 95% of intervals that include the true difference in means in the population, then the true difference in means isn't 0.

**Note:** The negative sign on *t* just tells us which way round the difference in means was calculated: either male - female or female - male. Here, we can see in the output that the means are given in the order male, then female, probably because of the numerical order of the original codes (0,1). That means that the difference in means was calculated as `r nars.t$estimate[1] %>% round(2)` - `r nars.t$estimate[2] %>% round(2)` = `r (nars.t$estimate[1] - nars.t$estimate[2]) %>% round(2)`, so *t* is also negative.
</details>

\ 

<details>
<summary>Write-up and Reporting</summary>

The following is an **example** of how you could write up these results. You should start practicing writing in your own words, since you will need to do this for your report.

<pre><code>
We next investigated whether male and female participants gave different ratings on the Negative Attitude towards Robots Scale (NARS; Nomura, Kanda, & Suzuki, 2006), beginning with the first subscale, negative attitudes toward situations of interaction with robots. Male participants had an average total score of &#96;r nars.t$estimate[1] %>% round(2)&#96;/30, whereas female participants had an average total score of &#96;r nars.t$estimate[2] %>% round(2)&#96;/30. This difference was significant (*M*~diff~ = &#96;r (nars.t$estimate[1] - nars.t$estimate[2]) %>% round(2)&#96;, 95% CI [&#96;r nars.t$conf.int[1] %>% round(2)&#96;, &#96;r nars.t$conf.int[2] %>% round(2)&#96;], *t* (&#96;r nars.t$parameter&#96;) = &#96;r nars.t$statistic %>% round(2)&#96;, *p* = &#96;r nars.t$p.value %>% rd(.,3)&#96;), indicating that - since a higher score on the NARS subscales indicates a more negative attitude - female participants on the whole felt more negatively towards interacting with robots.
</code></pre>

This will be rendered in Markdown as:

We next investigated whether male and female participants gave different ratings on the Negative Attitude towards Robots Scale (NARS; Nomura, Kanda, & Suzuki, 2006), beginning with the first subscale, negative attitudes toward situations of interaction with robots. Male participants had an average total score of `r nars.t$estimate[1] %>% round(2)`/30, whereas female participants had an average total score of `r nars.t$estimate[2] %>% round(2)`/30. This difference was significant (*M*~diff~ = `r (nars.t$estimate[1] - nars.t$estimate[2]) %>% round(2)`, 95% CI [`r nars.t$conf.int[1] %>% round(2)`, `r nars.t$conf.int[2] %>% round(2)`], *t* (`r nars.t$parameter`) = `r nars.t$statistic %>% round(2)`, *p* = `r nars.t$p.value %>% rd(.,3)`), indicating that - since a higher score on the NARS subscales indicates a more negative attitude - female participants on the whole felt more negatively towards interacting with robots.
</details>

## Lab Reports

If you have completed all the tasks above, congratulations! You now know everything you need to know to prepare the data, run, interpret, and report the analysis for your lab report if you are going to write about [the red study (Elliot et al., 2010)](https://canvas.sussex.ac.uk/courses/9242/pages/Lab%20Report%20Information%20and%20Resources#red). For your lab report, you should analyse and report your results exactly as we just practiced, using `t.test()`.

If you are planning to write about [the green study (Griskevicius et al, 2010)](https://canvas.sussex.ac.uk/courses/9242/pages/Lab%20Report%20Information%20and%20Resources#green), then you should review [last week's tutorial and practical on chi-squared](https://canvas.sussex.ac.uk/courses/9242/pages/week-6) instead.

\ 

\ 

The second bit of this tutorial will look briefly at how to run the same analysis using `lm()`. This is only a preview of the rest of the module after the lab reports are done, when we will look at the linear model in more depth.

## Comparing Means with `lm()`

As we saw in the lecture, we can capture the same relationship as a *t*-test using a linear model (LM). When we do this, we essentially draw a line connecting the two means. Let's have a look at how this works using the function for creating linear models, called (very sensibly) `lm()`.

`r task()`Open the help documentation for `lm()` and have a look under "Usage" and "Details" at how to use it.

You will see that `lm()` has a lot of different arguments. This is in part because `lm()` is so versatile, for so many different types of models, so it needs lots of different options. We can essentially ignore them all at the moment, though we might need more of them later on.

One of the reasons we used the `formula` method in `t.test()` is that `lm()` uses the exact same formula as input. Let's use the second *t*-test we did above, comparing NARS scale 1 in male vs female participants.

`r task()`Use `lm()` to analyse the difference between male and female participants in their mean NARS subscale 1 scores, and save the result as `nars.lm`.

```{r, toggle = T}
# The formula and data are the same; just t.test() has changed to lm()

nars.lm <- sync %>% 
  lm(nars1 ~ Gender, .)

# Indentical command, just without a pipe
nars.lm <- lm(nars1 ~ Gender, sync)
```

As before, we can call the name of the object to see what's in it:

```{r}
nars.lm
```

These numbers should look familiar from the *t*-test analysis we ran before. You may notice that the value labeled `(Intercept)` is the same as the mean score for male participants that we saw before, i.e. `r nars.t$estimate[1] %>% round(2)`. The value under `GenderFemale` tells us how much the estimated mean score changes when we move from the category `Male`, the intercept, to `Female`. This value is the same as the difference in means that we saw before, namely `r (nars.t$estimate[1] - nars.t$estimate[2]) %>% round(2) %>% abs()`. Essentially, this model represents the following situation:

```{r}
bracket <- sync %>%
  group_by(Gender) %>%
  summarise(y = mean(nars1)) %>%
  mutate(x = rep(2.7, 2),
         y = round(y, 2))
  
sync %>%
  ggplot(aes(Gender, nars1)) +
  geom_line(stat="summary", fun.y = mean, aes(group = NA)) +
  geom_errorbar(stat = "summary", fun.data = mean_cl_boot, width = .25) +
  geom_point(stat = "summary", fun.y = mean, shape = 21, size = 5, fill = "#008080") +
  geom_text(aes(label=round(..y..,2)), stat = "summary", fun.y = mean, size = 4,
             nudge_x = c(-.3, .3)) +
  labs(x = "Participant Gender", y = "Mean NARS Subscale 1 Score") +
  geom_line(data = bracket, mapping = aes(x, y)) +
  geom_segment(data = bracket, mapping = aes(x, y, xend = x - .05, yend = y)) +
  annotate("text", x = bracket$x + .1, y = min(bracket$y) + diff(bracket$y)/2,
           label = paste0("Difference:\n", bracket$y[2], " - ", bracket$y[1], "\n= ", diff(bracket$y)),
           hjust=0) + 
  ylim(0, 30) +
  coord_cartesian(xlim = c(0.5, 3.5)) +
  theme_cowplot()
  
```

<!-- <center> -->
<!-- ![](nars_plot_line.jpg) -->
<!-- </center> -->

Since I said above that `lm()` is very versatile and has lots of options, it may be surprising that the output we saw above is so simple, with only two numbers in it. If you're happy with that, that's fine - we'll dig a bit deeper in a couple of weeks. However, just like we've seen with `htest` objects, there's more stored in this object than gets printed out when we call it. 

`r task()`**Optional:** If you're interested, trying piping your saved LM output into `class` to see what type of object it is, and into `str()` and `summary()` to see what's in it.

<details>
<summary>Solution</summary>
```{r}
nars.lm %>% class()

nars.lm %>% str()

nars.lm %>% summary()
```

First, we have a new class of object, `lm`, which is unsurprising; this class of object contains linear models.

As for the rest, it looks like what we saw above was just the tip of the iceberg! We will look at some of these values and options in future weeks. For now, just notice that in the output from `summary()`, we get a *t*-value and *p*-value for `GenderFemale` that should also look familiar from above!
</details>

\ 

## Recap

You made it! That was, again, a lot of information, but remember that this tutorial is as much a reference for you in the future as it is for you to practice right now. You're welcome - and encouraged! - to keep working with this data to practice what we've learned today. In sum, we've covered:

* How to prepare and visualise data for comparing two means
* How to compare means using `t.test()`, read the output, and store the results as an `htest` object
* How to interpret and write up the results and output from `t.test()`
* How to compare means using `lm()`, briefly

That's all for this week. See you soon!

\ 