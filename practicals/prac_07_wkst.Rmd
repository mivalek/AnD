---
title: "Practical 7: Comparing Means"
author: "Analysing Data"
---

```{r setup, include=FALSE, message=F, warning=F}
library(tidyverse)
library(kableExtra)
library(cowplot)
library(weights)
library(Hmisc)
knitr::opts_chunk$set(echo = T, fig.height=4, fig.width=5, fig.align = "center", message = F, warning = F, toggle = F, sol = T)
```

This practical will help you practice preparing for, conducting, and reporting analyses that compare means between two groups in R. Before you start this practical, you should make sure you review the relevant lecture, as this practical assumes you already know what these analyses are and how to interpret the output. You should also use this practical in combination with the tutorial on the same topic.

## Setting Up

For this practical, you will need this worksheet, RStudio, and a new Markdown document. Remember that you can easily switch between windows with the <kbd>Alt</kbd> + <kbd>&#8633; Tab</kbd> (Windows) and <kbd>&#8984;\ Command</kbd> + <kbd>&rarrb; Tab</kbd> (Mac OS) shortcuts.

`r task()`Open your analysing_data `R` project in RStudio and open a new Markdown file. Since we will be practicing reporting using inline code, we will need Markdown later on. For the tasks, get into the habit of creating new code chunks as you go.

\ 

`r task()`Aside from loading `tidyverse`, we will also be using `cowplot` and `kableExtra`. If you don't have any of these packages installed, do that now. **Remember that installing packages is a one-off thing** so don't put the command that does it in your Markdown. Simply type it into the console and press <kbd>&crarr;\ Enter</kbd>. The `library()` commands should always go in a separate code chunk at the beginning of your Markdown document, so that your code will run correctly.

```{r, eval=F, echo = solution}
library(tidyverse)
library(Hmisc)
library(here)
library(kableExtra)
library(cowplot)
```

\ 

`r task()`As usual, we will need some data to work with. Since it's already cleaned up and we had a look at it, let's use the harm dataset again that we have seen previously, from [an actual paper by Swiderska and KÃ¼ster (2018)](https://journals.sagepub.com/doi/10.1177/0301006618809919). You can find the data from the study at https://raw.githubusercontent.com/mivalek/AnD/master/data/harm_data.csv. You will also need to copy and paste in the code that you wrote from Practical 3, and/or that you used in the last practical, to clean this dataset.

**Hint**: Copy and paste in code that saves something or makes a change to the dataset, ie uses the assignment operator `<-`.

```{r, echo = solution}
data <- read_csv("https://raw.githubusercontent.com/mivalek/AnD/master/data/harm_data.csv")

data <- data %>%
  filter(Age != "1y") %>% # filter out row with typo in Age
  mutate(Age = as.numeric(Age), # turn into numeric
         ID = factor(ID),
# Make sure you have the right order of factors for Condition and Harm
         Condition = factor(Condition, labels = c("Human_Harmed", "Human_Unharmed", "Robot_Harmed", "Robot_Unharmed")),
         Humanness = factor(Humanness, labels = c("Human", "Robot")),
         Harm = factor(Harm, labels = c("Harmed", "Unharmed")),
         Gender = factor(Gender, labels = c("Male", "Female")))

# Number of px we're about to remove due to outlying/unlikely ages
too_young <- data %>%
  filter(Age < 17) %>%
  nrow()
too_old <- data %>%
  filter(Age > 90) %>%
  nrow()

# remove cases
data <- data %>% filter(Age > 18 & Age < 90)
```

\ 

## Preparation

As usual, you should remind yourself of what these variables actually mean before you start, using the codebook below.

As a reminder, in the original study the researchers showed each participant one avatar, which could have been one of four possibilities. First, the avatar could have been either a human or robot avatar [variable `Humanness`]. Second, the avatar could have been either harmed or unharmed [variable `Harm`]. So, there were four possible conditions, each representing one of the four possible avatars - namely, `Human_Unharmed`, `Human_Harmed`, `Robot_Unharmed`, and `Robot_Harmed` [variable `Condition`]. The study hypothesised that participants would rate the avatars more highly on `Experience` - that is, whether the avatar had feelings, desires, etc. - if they were harmed, with similar hypotheses for `Agency` and `Consciousness`. They also thought that participants would rate the avatars more highly on experience, agency, and consciousness if they thought the avatars had greater capacity to feel pain [variable `Pain`].

`r task()` Given the study description above, and with reference to the codebook, write down at least one **operational** hypothesis that we could test with this data. This should define both the predictor and outcome variables, and what the results of the test will show you.

### Codebook

```{r, echo = F}
read_csv("harm_codebook.csv") %>% 
  kable() %>% 
  kable_styling()
```

<!--solution
Based on the description above and the original paper, there are several possible hypotheses we could propose. To begin, we could investigate the relationships between `Harm` and `Experience`, which was one of the original researchers' main interests. We could state this hypothesis as follows:

> Participants who saw an avatar that had been harmed will give different ratings of that avatar's mental experiences than participants who saw an avatar that was unharmed.

In other words, we are hypothesizing that the mean ratings of `Experience` will differ between the two levels of `Harm`. We could also make a similar hypothesis about `Humanness`:

> Participants who saw a human avatar will give different ratings of that avatar's mental experiences than participants who saw a robotic avatar.

Again, we are hypothesizing that the mean ratings of `Experience` will differ between the differ between the two levels of `Humanness`.

There are many more hypotheses of the same sort you could make using this dataset. We could again hypothesize different levels of `Pain`, or `Consciousness`, between the different levels of `Harm`, or `Humanness`. All of these you are welcome to try out yourself!
-->

\ 

For the time being, we will focus on the mean ratings of `Experience` in different `Harm` and `Humanness` conditions. Overall, our goal is to find out whether the type of avatar that the participant saw influenced their rating of that avatar's mental experiences.

## Graphing

Once again, we should graph our data before we jump into running our analysis. Looking at summaries of numbers is good, but there's no substitute for a good graph to help you understand your data.

`r task()`Create a nicely formatted means plot, with error bars, of `Experience`, split up by `Harm`, and save it as `harm.means.plot`.

**Hint**: See the tutorial for how to do this. You are welcome to customise the plot (e.g. colour and shape) however you like.

```{r, echo = solution}
harm.means.plot <- data %>% 
  ggplot(aes(x = Harm, 
             y = Experience)) +
  labs(x = "Avatar Harm Condition", 
       y = "Mean Rating of Experience") +
  stat_summary(fun.data="mean_cl_boot", 
               geom="errorbar", 
               width = .25) +
  geom_point(stat = "summary",
           fun.y = "mean",
           size = 3, # chosen at random - larger numbers make bigger points
           shape = 21,
           fill = "dark red") + # or whatever colour you chose!
  ylim(0, 7) +  # Don't forget to change this to match your rating scales
  theme_cowplot()

harm.means.plot
```

`r subtask()`Before you move on, stop and think about what the means plot tells you about our research question.

<!--solution
We can see that the means are definitely different from each other. In particular, the mean for the `Harmed` condition is noticeably higher than the mean in the `Unharmed` group. If you refer back to the original research question, this was indeed exactly what the researchers predicted.
-->

`r task()`Create a nicely formatted means plot, with error bars, of `Experience`, split up by `Humanness`, and save it as `human.means.plot`.

**Hint**: See the tutorial for how to do this. You are welcome to customise the plot (e.g. colour and shape) however you like.

```{r, echo = solution}
human.means.plot <- data %>% 
  ggplot(aes(x = Humanness, 
             y = Experience)) +
  labs(x = "Avatar Humanness Condition", 
       y = "Mean Rating of Experience") +
  stat_summary(fun.data="mean_cl_boot", 
               geom="errorbar", 
               width = .25) +
  geom_point(stat = "summary",
           fun.y = "mean",
           size = 3, # chosen at random - larger numbers make bigger points
           shape = 25,
           fill = "dark green") + # or whatever colour you chose!
  ylim(0, 7) + # Don't forget to change this to match your rating scales
  theme_cowplot()

human.means.plot
```

`r subtask()`Before you move on, stop and think about what this means plot tells you about our research question.

<!--solution
We can see that again, the means are definitely different from each other. Human avatars were rated much higher on Experience than robot avatars - which is exactly what we might expect! Comparing the two plots, the difference in means here, for `Humanness`, is clearly larger than the difference in means for `Harm`, above. We can also see that not only were human avatars rated more highly than robot avatars, they were also rated more **consistently**. We know this because the confidence intervales for the `Human` condition are narrower (closer together) than in the `Robot` condition.
-->

These two plots give us a clear idea of how participants rated the avatars in the two `Harm` and `Humanness` conditions. However, these conditions were combined in the original study, and there may be a combined effect of both `Harm` and `Humanness` on ratings. Let's create a plot to visualise this.

`r task()`Create a nicely formatted means plot, with error bars, of `Experience`, split up by `Harm`, and faceted on `Humanness`. Save it as `int.means.plot`.

**Hint**: Use `facet_wrap(~...)` to get separate plots. You are welcome to customise the plot (e.g. colour and shape) however you like.

```{r, echo = solution}
int.means.plot <- data %>% 
  ggplot(aes(x = Harm, 
             y = Experience)) +
  facet_wrap(~Humanness) +
  labs(x = "Avatar Humanness Condition", 
       y = "Mean Rating of Experience") +
  stat_summary(fun.data="mean_cl_boot", 
               geom="errorbar", 
               width = .25) +
  geom_point(stat = "summary",
           fun.y = "mean",
           size = 3, # chosen at random - larger numbers make bigger points
           shape = 22,
           fill = "purple") + # or whatever colour you chose!
  ylim(0, 7) + # Don't forget to change this to match your rating scales
  theme_cowplot()

int.means.plot
```

`r subtask()`Before you move on, stop and think about what this means plot tells you about our research question.

<!--solution
Now that we are looking at the combination of both the `Harm` and `Humanness` conditions, we can we the real pattern of results. Overall, both of the means for humans (left side plot) are higher than both of the means for robots (right side plot); this is the same pattern that we saw in the plot we made above for `Humanness` only. However, we can also see that within human and robot avatars, the means were different depending on whether the avatar was harmed or unharmed. Specifically, harmed avatars were rated higher on average experience than unharmed avatars. So, the rating of Experience depended on **both** `Harm` and `Humanness`.
-->

`r subtask()`**CHALLENGE**: Create the same `int.means.plot` as above, but try to fill the points differently for Harmed vs Unharmed avatars. You should also remove the legend.

**Hint**: You will need to add an element to your `aes()` variable mapping, and have a look at `scale_fill_manual()`.

```{r, echo = solution}
data %>% 
  ggplot(aes(x = Harm, 
             y = Experience,
             fill = Harm)) +
  facet_wrap(~Humanness) +
  labs(x = "Avatar Humanness Condition", 
       y = "Mean Rating of Experience") +
  stat_summary(fun.data="mean_cl_boot", 
               geom="errorbar", 
               width = .25) +
  geom_point(stat = "summary",
           fun.y = "mean",
           size = 3, # chosen at random - larger numbers make bigger points
           shape = 23) + # or whatever colour you chose!
  scale_fill_manual(values = c("dark red", "dark blue"), guide = FALSE) +
  ylim(0, 7) + # Don't forget to change this to match your rating scales
  theme_cowplot()
```

<!--solution
Now that's a snazzy plot! Nicely done.
-->

## Using `t.test()`

Now that we have an idea of the pattern of the means, we can choose do some analyses to find out whether these differences are statistically significant.

`r task()`Using the formula method, run a *t*-test comparing the mean ratings of `Experience` in different `Harm` groups and save it as `harm.t`.

```{r, echo = solution}
harm.t <- data %>% 
  t.test(Experience ~ Harm, ., alternative = "two.sided", var.equal = T)
harm.t
```

`r subtask()`Report the results of this test in APA style, using subsetting and inline code to report values from the output.

<!--solution
<pre><code>
We investigated whether participants would give different ratings of an avatar's mental experiences depending on harm condition, using an independent-measures *t*-test. The analysis indicated that, as predicted, harmed avatars were given higher mean ratings of experience (*M* = &#96;r harm.t$estimate[1] %>% round(2)&#96;) than unharmed avatars (*M* = &#96;r harm.t$estimate[2] %>% round(2)&#96;). This difference in means was significant (*M*~diff~ = &#96;r (harm.t$estimate[1] - harm.t$estimate[2]) %>% round(2)&#96;, 95% CI [&#96;r harm.t$conf.int[1] %>% round(2)&#96;, &#96;r harm.t$conf.int[2] %>% round(2)&#96;], *t* (&#96;r harm.t$parameter&#96;) = &#96;r harm.t$statistic %>% round(2)&#96;, *p* = &#96;r harm.t$p.value %>% rd(.,3)&#96;).
</code></pre>

This will be rendered in Markdown as:

We investigated whether participants would give different ratings of an avatar's mental experiences depending on harm condition, using an independent-measures *t*-test. The analysis indicated that, as predicted, harmed avatars were given higher mean ratings of experience (*M* = `r harm.t$estimate[1] %>% round(2)`) than unharmed avatars (*M* = `r harm.t$estimate[2] %>% round(2)`). This difference in means was significant (*M*~diff~ = `r (harm.t$estimate[1] - harm.t$estimate[2]) %>% round(2)`, 95% CI [`r harm.t$conf.int[1] %>% round(2)`, `r harm.t$conf.int[2] %>% round(2)`], *t* (`r harm.t$parameter`) = `r harm.t$statistic %>% round(2)`, *p* = `r harm.t$p.value %>% rd(.,3)`).
-->

`r subtask()`Explain in your own words what these results tell you about the relationship between `Harm` and `Experience`.

<!--solution
As we learned in the lecture, the null hypothesis of the *t*-test is that the two sample means come from the same population - that is, which group you are in (here, whether you saw a harmed or unharmed avatar) makes no difference to experience ratings. If this is the case, and the null hypothesis is true, the difference in sample means should be 0. As we can see here, the difference in sample means is not 0 - it's `r (harm.t$estimate[1] - harm.t$estimate[2]) %>% round(2)`, on our scale of 0 - 7. So, is this a "big enough" difference to believe that that our two sample means **do not** come from the same population?

To evaluate this, our analysis compares the difference in means that we found (the signal) to the standard deviation of the differences in means (the noise) to produce our test statistic *t*. The larger *t* is, the stronger the signal is compared to the noise, and the less likely it is to occur if the null hypothesis is true. For the data we have, the probability of finding a value of *t* as big as `r harm.t$statistic %>% round(2)`, or bigger, is very small. Namely, the probability *p* of this occuring is `r harm.t$p.value %>% rd(.,7)`, as we can see in the output; or stated more simply, *p* = `r harm.t$p.value %>% rd(.,3)`. Since this probability is quite small, it is therefore very unlikley that our two sample means come from the same population (reject the null). Instead, we conclude that they may come from different populations (accept the alternative). In this case, the "two different populations" would be "people who saw a harmed avatar" and "people who saw an unharmed avatar".

In short, the result of our analysis suggests that whether a participant saw a harmed or unharmed avatar does influence their ratings of that avatar's mental experiences.
-->

## Using `lm()`

As we saw in the lecture and tutorial, we can also compare differences in means using a linear model, or LM. Let's try doing this with the other predictor of interest, `Humanness`.

`r task()`Create a linear model capturing the relationship between `Humanness` and `Experience`, and save it as `human.lm`.

```{r, echo = solution}
human.lm <- data %>% 
  lm(Experience ~ Humanness, data = .)
human.lm
```

`r subtask()`How can you interpret both of the numbers in this output?

**Hint**: Calculate the mean ratings of Experience in the Human and Robot conditions separately, and the difference between them.

<!--solution
Remember that this linear model simply describes the same relationship we saw before. Have another look at the means plot we made earlier:

```{r, echo = F}
human.means.plot
```

We can see that the first, baseline category, Human, has a mean of about 6. In fact, the mean rating of Experience in our dataset for people who saw a human avatar was `r data %>% filter(Humanness == "Human") %>% pull(Experience) %>% mean() %>% round(2)`. This is the same as the **intercept**, or the value at the start of the line. 

Then we can see that when we change from Human avatars to Robot avatars, the estimated mean rating of Experience changes to `r data %>% filter(Humanness == "Robot") %>% pull(Experience) %>% mean() %>% round(2)`. In other words, it decreases by 2.81. This is the difference in means, or the **slope** of the line.

So, from the `lm()` output we can get three useful pieces of information:

* The sample mean in the Human group, labeled `(Intercept)`
* The difference in means between the Human and Robot groups, labeled `HumannessRobot`
* The sample mean in the Robot group, which we can find by simply adding the difference in means to the baseline (Human) mean: 6.08 + (-2.81) = 3.27.

If you wanted to confirm this, you can use `summarise` to produce the means for you:

```{r}
data %>% 
  group_by(Humanness) %>% 
  summarise(
    mean_exp = mean(Experience),
    sd_exp = sd(Experience)
  ) %>% 
  kable(caption = "Mean Experience Ratings",
        col.names = c("Humanness Condition", "*M*~Experience~", "*SD*~Experience~"),
        digits = 2) %>% 
  kable_styling()
```
-->

`r subtask()`**CHALLENGE**: Run the following code and have a look at the output. Is the relationship between `Humanness` and `Experience` significant?

```{r}
human.lm %>% summary()
```

<!--solution
There is a lot of information here, so look for what you know. We can recognise `(Intercept)` and `HumannessRobot` from the output we saw before, and we can also recognise the same numbers that we saw before under `Estimate`. 

The line we are really interested in here is `HumannessRobot`, which tells us about what happens when we change from human to robot avatars - i.e. it tells us how the means change, which is what we want to know. In this output, we can see something else we might recognise: a column labeled `t value`. Aha, it's our old friend *t* again! The value for *t* is pretty big, namely -10.99. Is this big enough to believe that human and robot avatars were actually given different ratings of mental experiences? 

In the next column over, we have `Pr(>|t|)`. This looks like a keysmash, but you can read it as "Probability of the absolute value of *t* (or greater)". That should sound familiar - this is our *p*-value. As we've seen before, when R is giving us values that are very very very small, it gives us this strange value "<2e-16", which you should read as "< .001". 

As an extra note, notice that next to this *p*-value there are three asterisks. Directly underneath, R tells you how read these in the line "Signif. codes", which means "Significance codes". In this line you can see that the number of asterisks corresponds to different values of *p*. We can read these codes as follows:

* *p* < .001: ***
* .001 < *p* < .01: **
* .01 < *p* <.05: *
* .05 < *p* < .01: .
* .01 < *p*: [nothing]

Essentially, these are a quick visual reference to help you interpret *p*-values quickly, which is especially useful if you are looking at lots of them at once!
-->

## Summary

At this point, you have practiced interpreting the output of two different methods of comparing means: `t.test()` and `lm()`. If you want more practice with this, you have an entire dataset to play with, and you are welcome to try running these analyses on different combinations of predictors and outcomes of your choosing. At this point, we have practiced everything you need to know for this module, so you can stop here if you like.

**Optionally**, if you are keen to know the answer to our original research question - that is, how are mean ratings of `Experience` different in different combination of conditions - we will look at that in the final part of this practical, but that will bring us back around to interactions and face to face with familywise error.

## A Closer Look

`r task()`Have another look at the interaction plot we made earlier and saved in `int.means.plot`. Based on the error bars, which means do you think will be significantly different from each other?

```{r, echo = F}
int.means.plot
```

<!--solution
Remember that the confidence intervals give us an idea of how spread out the scores are around the mean. Narrower confidence intervals indicate that the ratings were more consistent, with less variation, and therefore the mean is a better estimate. Conversely, wider confidence intervales indicate that the ratings were less consistent, with more variation, and therefore the mean is not as good an estimate.

Remember as well that we can interpret confidence intervals as a range of possible values where we expect the true population value to fall. For any of the means above, if it is one of the 95% of intervals that does contain the true population mean, than that true value of the mean will fall somewhere between the upper and lower bounds of the interval. We can use this to go a step further: if the confidence intervals for two different means don't overlap (that is, they don't contain any of the same values), than it is likely that the true values of those means are not the same. If they do substantially overlap, then it's possible the true values of the means are similar, or the same. As a rule of thumb, overlap of 25% or less of the width of the confidence interval typically corresponds to a significant difference between the means.

With this in mind, look at the plot again. Within the Robot condition, we can see that the means in the harmed vs unharmed condition overlap quite a bit. It's not clear whether this is more or less than 25%, so we would have to test that difference, using the techniques we've practiced above, to be sure. Within the Human condition, it looks like the confidence intervals don't overlap at all, or barely, because the lower bound of the Harmed condition confidence interval and the upper bound of the Unharmed confidence interval are almost exactly in line. Again, though, since significance is a function of sample size, we would need a test to make sure.

All of the other means are very different from each other and their confidence intervals do not overlap at all. So, we might expect that every other comparison between means would result in a significant diffference.
-->

`r task()`Use a *t*-test to compare the difference in mean ratings of Experience between the `Robot_Harmed` and the `Robot_Unharmed` condition. What can you conclude?

<!--solution
```{r}
robot.harm.t <- data %>% 
  filter(Humanness == "Robot") %>% 
  t.test(Experience ~ Harm, data = ., alternative = "two.sided", var.equal = T)

robot.harm.t
```
Just as we suspected from the confidence intervals, this difference is not significant, but it is extremely close! This is a good example of why eyeballing confidence interval overlap is a useful rule of thumb, but requires follow-up testing.

The analysis shows us that for people who saw robot avatars, the mean ratings of experience were somewhat higher for harmed avatars (*M* = `r robot.harm.t$estimate[1] %>% round(2)`) than for unharmed avatars (*M* = `r robot.harm.t$estimate[2] %>% round(2)`). However, this difference in means was not significant (*M*~diff~ = `r (robot.harm.t$estimate[1] - robot.harm.t$estimate[2]) %>% round(2)`, 95% CI [`r robot.harm.t$conf.int[1] %>% round(2)`, `r robot.harm.t$conf.int[2] %>% round(2)`], *t* (`r robot.harm.t$parameter`) = `r robot.harm.t$statistic %>% round(2)`, *p* = `r robot.harm.t$p.value %>% rd(.,3)`). So, for robotic avatars, whether they were harmed or unharmed did not significantly influence ratings of mental experience.

(Note that not only is the *p*-value greater than .05, but the confidence interval around the mean difference includes 0. This suggests that if this is one of the 95% of confidence intervals that contain the true population value **of the difference in means**, then the true difference in means could be 0, i.e. no difference at all. It's a bit confusing, but this confidence interval around the difference in means is different from either of the confidence intervals we were looking at before on the plot, which were around each of the means individually.)
-->

This is a useful thing to know, but we have tested only one pair of conditions, harmed robots vs unharmed robots. Because there are four possible combinations (harmed and unharmed humans and robots), to test all pairs of conditions against each other we would have to do 6 total *t*-tests. Not only does that sound exhausting, it's extremely inadvisable - in fact, you should **never** do multiple *t*-tests within the same dataset like this. The reason why is briefly explained below.

\ 

<div class = "warn">
### Multiple Comparisons

To find out which means in which conditions are different from each other, it seems like the best idea is just compare all combinations of conditions. So, we could start by comparing harmed vs unharmed robots, as we did above. Then, we could compare harmed vs unharmed humans. Then, harmed humans vs harmed robots, then unharmed humans vs unharmed robots, then unharmed humans vs harmed robots, and finally harmed humans vs unharmed robots. We'd have a whole lot of *p*-values to report but then we would know the answer to our original research question...Right?

Not really. Besides the fact that this is repetitive, it doesn't really capture our research question correctly. Doing the analysis this way treats each pair of means as independent, which they aren't - because they all come from the same dataset. For example, we could do two *t*-tests comparing harmed vs unharmed robots, and harmed robots vs harmed humans. But in both tests, we would use the same data: the "harmed robots" group is the same in both comparisons. More importantly, though, we're coming up against a dramatic inflation of our Type 1 error rate.

The essential problem is that by making lots of comparisons like this, we increase the probability that we will find a statistically "significant" result (that is, a *p*-value lower than .05) that doesn't actually mean anything. This is Type 1 error, or a false positive. The more comparisons we make, the more likely a false positive becomes. The graph below shows the probability of Type 1 error for the number of comparisons we make:

```{r, echo = F}
type1prob <- NULL

for(x in c(1:20)){
    prob <-(1 - (.95^x))
    type1prob <- c(type1prob, prob)
}

typeone <- tibble(
  Comparisons = factor(c(1:20)),
  Type1Prob = type1prob)

typeone %>% 
  ggplot(aes(Comparisons, Type1Prob)) +
  geom_point() +
  labs(x = "Number of Comparisons", y = "Probability of Type 1 Error") +
  ylim(0, 1) +
  theme_cowplot()
```
We can see that when we make 6 comparisons, there is about a 25% chance that we will get *at least one* statistically significant result that is actually a false positive.

If you haven't really understood why this is the case, that's fine - we will come back to this again in second year and explain in more depth. For now, the main takeaway is that doing multiple uncorrected *t*-tests, like we suggest above, is very bad practice and can lead to completely wrong, misrepresentative conclusions.
</div>

## Correcting for Multiple Comparisons

What we would really like is a single model that will capture the effect of `Harm` when we ignore `Humanness`; the effect of `Humanness`, when we ignore `Harm`; **and** the combined effect of `Harm` and `Humanness` together. We cannot do this with *t*-tests, because each *t*-test only compares pairs of means, and we have four means that we want to compare to each other. We can do it easily with the linear model...but we'll get to that in the future!

For now, instead, we can **adjust our significance level** based on the number of comparisons we make. Essentially, the more familywise (ie related) comparisons we make, the more strict our criteria is for significance. The easiest, and also the most stringent, method of correction is called the Bonferroni correction, and it's very simple: divide the significance level by the number of comparisons you will make to obtain your new significance level.

`r task()`Use R to calculate the Bonferroni-adjusted significance level for the comparisons in our dataset and save it as `bonferroni.sig`.

```{r, echo = solution}
bonferroni.sig <- 0.05/6 # significance level divided by six comparisons
bonferroni.sig
```

This means that we will no longer accept *p* < .05 as significant. To reject the null hypothesis, we require stronger evidence; instead, for each of these six comparisons, *p* must be smaller than `r bonferroni.sig %>% rd(.,3)`.

`r task()`Calculate *t*-tests for each of the six comparisons between conditions for ratings of Experience. Which pairs of means are significantly different from each other, using our corrected significance level?

<!--solution
```{r}
comp1 <- data %>% 
  filter(Humanness == "Robot") %>% 
  t.test(Experience ~ Harm, data = ., alternative = "two.sided", var.equal = T)

comp2 <- data %>% 
  filter(Humanness == "Human") %>% 
  t.test(Experience ~ Harm, data = ., alternative = "two.sided", var.equal = T)

comp3 <- data %>% 
  filter(Harm == "Harmed") %>% 
  t.test(Experience ~ Humanness, data = ., alternative = "two.sided", var.equal = T)

comp4 <- data %>% 
  filter(Harm == "Unharmed") %>% 
  t.test(Experience ~ Humanness, data = ., alternative = "two.sided", var.equal = T)


# Extracting specific subsets of the data
hum.unharm.exp <- data %>% 
  filter(Condition == "Human_Unharmed") %>% 
  pull(Experience)

hum.harm.exp <- data %>% 
  filter(Condition == "Human_Harmed") %>% 
  pull(Experience)

robot.unharm.exp <- data %>% 
  filter(Condition == "Robot_Unharmed") %>% 
  pull(Experience)

robot.harm.exp <- data %>% 
  filter(Condition == "Robot_Harmed") %>% 
  pull(Experience)

# Last two comparisons run using the extracted data
comp5 <- t.test(hum.unharm.exp, robot.harm.exp, alternative = "two.sided", var.equal = T)

comp6 <- t.test(hum.unharm.exp, robot.harm.exp, alternative = "two.sided", var.equal = T)

comp1$p.value < bonferroni.sig
comp2$p.value < bonferroni.sig
comp3$p.value < bonferroni.sig
comp4$p.value < bonferroni.sig
comp5$p.value < bonferroni.sig
comp6$p.value < bonferroni.sig
```
The output tells us that after adjusting for multiple comparisons, the differences between all pairs of conditions were still significant, except for comparison 1 between harmed and unharmed robots, which we already saw above. If you are interested, you can go back and look at each of these *t*-tests individually and report the results using the saved output and the graph we already made.
-->

We will return to the issue of multiple comparisons, and how to construct a better model to capture this complex relationship between predictors, in the future. For now, well done!

\ 

## Recap

In sum, today we've practiced:

* How to prepare and visualise data for comparing means
* How to conduct a *t*-test, and report and interpret the output
* How to construct a linear model, and report and interpret the output
* How (and why!) to correct for Type 1 error inflation

That's all for today. See you soon!

\ 